{"cells":[{"cell_type":"code","execution_count":1,"id":"2807aa3f","metadata":{"id":"2807aa3f","executionInfo":{"status":"ok","timestamp":1731215295410,"user_tz":-630,"elapsed":32173,"user":{"displayName":"Anik Balasubramannian","userId":"12375949825035405902"}}},"outputs":[],"source":["\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","import matplotlib.pyplot as plt\n","import numpy as np\n"]},{"cell_type":"code","execution_count":2,"id":"40381277","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40381277","executionInfo":{"status":"ok","timestamp":1731215310612,"user_tz":-630,"elapsed":8905,"user":{"displayName":"Anik Balasubramannian","userId":"12375949825035405902"}},"outputId":"650b330c-458e-4e31-cb74-b1602b63dcc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:03<00:00, 48.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}],"source":["\n","# Data augmentation and normalization for CIFAR-10\n","train_transforms = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n","])\n","\n","# Normalization for validation\n","valid_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n","])\n","\n","# Loading CIFAR-10 dataset\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\n","valid_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=valid_transforms)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)\n"]},{"cell_type":"code","execution_count":3,"id":"b0428d3e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b0428d3e","executionInfo":{"status":"ok","timestamp":1731215317550,"user_tz":-630,"elapsed":377,"user":{"displayName":"Anik Balasubramannian","userId":"12375949825035405902"}},"outputId":"3c404246-fd5d-46c3-8f2c-e48f8047a603"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]}],"source":["\n","# Initializing a ResNet-18 model from scratch (no pre-trained weights)\n","model = models.resnet18(pretrained=False)\n","num_ftrs = model.fc.in_features\n","\n","# Customizing the fully connected layer for the Plant Seedlings dataset\n","model.fc = nn.Sequential(\n","    nn.Linear(num_ftrs, 256),\n","    nn.ReLU(),\n","    nn.Dropout(0.4),\n","    nn.Linear(256, len(train_dataset.classes))\n",")\n","\n","# Transfer the model to GPU if available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n"]},{"cell_type":"code","execution_count":4,"id":"2752a1f9","metadata":{"id":"2752a1f9","executionInfo":{"status":"ok","timestamp":1731215322591,"user_tz":-630,"elapsed":392,"user":{"displayName":"Anik Balasubramannian","userId":"12375949825035405902"}}},"outputs":[],"source":["\n","# Setting up the criterion and different optimizers for experimentation\n","criterion = nn.CrossEntropyLoss()\n","optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n","optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"]},{"cell_type":"code","execution_count":5,"id":"43a4fa09","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43a4fa09","executionInfo":{"status":"ok","timestamp":1731233541344,"user_tz":-630,"elapsed":18215219,"user":{"displayName":"Anik Balasubramannian","userId":"12375949825035405902"}},"outputId":"9be62afb-7e62-4906-c834-202118b9cea3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 1.6522, Accuracy: 0.4067\n","Epoch 2/10, Loss: 1.3241, Accuracy: 0.5421\n","Epoch 3/10, Loss: 1.1329, Accuracy: 0.6162\n","Epoch 4/10, Loss: 1.0144, Accuracy: 0.6584\n","Epoch 5/10, Loss: 0.9226, Accuracy: 0.6900\n","Epoch 6/10, Loss: 0.8488, Accuracy: 0.7168\n","Epoch 7/10, Loss: 0.7974, Accuracy: 0.7359\n","Epoch 8/10, Loss: 0.7514, Accuracy: 0.7485\n","Epoch 9/10, Loss: 0.7032, Accuracy: 0.7633\n","Epoch 10/10, Loss: 0.6693, Accuracy: 0.7751\n","Epoch 1/10, Loss: 0.5979, Accuracy: 0.7981\n","Epoch 2/10, Loss: 0.5461, Accuracy: 0.8161\n","Epoch 3/10, Loss: 0.5233, Accuracy: 0.8241\n","Epoch 4/10, Loss: 0.5050, Accuracy: 0.8293\n","Epoch 5/10, Loss: 0.4952, Accuracy: 0.8329\n","Epoch 6/10, Loss: 0.4834, Accuracy: 0.8366\n","Epoch 7/10, Loss: 0.4693, Accuracy: 0.8416\n","Epoch 8/10, Loss: 0.4605, Accuracy: 0.8448\n","Epoch 9/10, Loss: 0.4455, Accuracy: 0.8489\n","Epoch 10/10, Loss: 0.4400, Accuracy: 0.8507\n"]}],"source":["\n","def train_model(model, criterion, optimizer, num_epochs=10):\n","    model.train()\n","    train_loss, train_acc = [], []\n","\n","    for epoch in range(num_epochs):\n","        running_loss, correct, total = 0.0, 0, 0\n","\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        epoch_acc = correct / total\n","        train_loss.append(epoch_loss)\n","        train_acc.append(epoch_acc)\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n","\n","    return train_loss, train_acc\n","\n","# Train the model with both optimizers for comparison\n","loss_adam, acc_adam = train_model(model, criterion, optimizer_adam)\n","loss_sgd, acc_sgd = train_model(model, criterion, optimizer_sgd)\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}